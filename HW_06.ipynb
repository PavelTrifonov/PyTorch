{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-v-DRUQ-Ic7F",
        "outputId": "b4ca7342-d738-49ff-92bb-8d1a6bab93eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.0.7)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.3)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PN6H7Ii4aMsC",
        "outputId": "232c5ded-e584-446c-d30f-57fbaf5b7d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 0.4767\n",
            "Epoch 2/5, Loss: 0.2864\n",
            "Epoch 3/5, Loss: 0.2756\n",
            "Epoch 4/5, Loss: 0.2709\n",
            "Epoch 5/5, Loss: 0.2674\n",
            "Example 1:\n",
            "Text: #passion to find <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Predicted Label: 0\n",
            "--------------------------------------------------\n",
            "Example 2:\n",
            "Text: @user #white want everyone to see the new #movie â and why <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Predicted Label: 0\n",
            "--------------------------------------------------\n",
            "Example 3:\n",
            "Text: safe ways to heal your #acne!! #altwaystoheal #healthy #healing!! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Predicted Label: 0\n",
            "--------------------------------------------------\n",
            "Example 4:\n",
            "Text: is the and the child book up for already? if yes, if no, ððð #harrypotter #favorite <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Predicted Label: 0\n",
            "--------------------------------------------------\n",
            "Example 5:\n",
            "Text: 3rd #bihday to my #nephew uncle dave loves you and <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Predicted Label: 0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Определите модель\n",
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_size, output_dim, dropout):\n",
        "        super(SentimentModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=filter_size)\n",
        "        self.fc = nn.Linear(num_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, embedding_dim) -> (batch_size, embedding_dim, seq_len)\n",
        "        x = self.conv1d(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max(x, dim=2)[0]  # Max pooling\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Создайте обратный словарь из vocab\n",
        "def get_index_to_token(vocab):\n",
        "    try:\n",
        "        index_to_token = vocab.get_itos()\n",
        "    except AttributeError:\n",
        "        index_to_token = {idx: token for token, idx in vocab.items()}\n",
        "    return index_to_token\n",
        "\n",
        "# Функция для создания словаря из данных\n",
        "def build_vocab(data, max_size=10000):\n",
        "    tokens = [token for text in data for token in text.split()]\n",
        "    token_counts = Counter(tokens)\n",
        "    most_common_tokens = token_counts.most_common(max_size - 2)\n",
        "    vocab = {token: idx + 2 for idx, (token, _) in enumerate(most_common_tokens)}\n",
        "    vocab['<unk>'] = 0  # Для неизвестных токенов\n",
        "    vocab['<pad>'] = 1  # Для заполнения (padding)\n",
        "    return vocab\n",
        "\n",
        "# Создайте класс Dataset для обучения\n",
        "class TrainTextDataset(Dataset):\n",
        "    def __init__(self, csv_file, vocab, max_len):\n",
        "        self.data = pd.read_csv(csv_file, header=0, names=['id', 'label', 'tweet'], dtype={'tweet': str})\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.data.iloc[idx]['tweet'])\n",
        "        label = self.data.iloc[idx]['label']\n",
        "        tokens = text.split()[:self.max_len]  # Ограничиваем длину текста\n",
        "        token_ids = [self.vocab[token] if token in self.vocab else self.vocab['<unk>'] for token in tokens]\n",
        "        token_ids = token_ids + [self.vocab['<pad>']] * (self.max_len - len(token_ids))  # Padding\n",
        "        return torch.tensor(token_ids), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Создайте класс Dataset для тестирования\n",
        "class TestTextDataset(Dataset):\n",
        "    def __init__(self, csv_file, vocab, max_len):\n",
        "        self.data = pd.read_csv(csv_file, header=0, names=['id', 'tweet'], dtype={'tweet': str})\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.data.iloc[idx]['tweet'])\n",
        "        tokens = text.split()[:self.max_len]  # Ограничиваем длину текста\n",
        "        token_ids = [self.vocab[token] if token in self.vocab else self.vocab['<unk>'] for token in tokens]\n",
        "        token_ids = token_ids + [self.vocab['<pad>']] * (self.max_len - len(token_ids))  # Padding\n",
        "        return torch.tensor(token_ids)\n",
        "\n",
        "# Функция для вывода примеров и предсказаний\n",
        "def show_predictions(model, data_loader, index_to_token, num_examples=5):\n",
        "    model.eval()\n",
        "    examples = []\n",
        "    with torch.no_grad():\n",
        "        for texts in data_loader:\n",
        "            for i in range(min(num_examples, len(texts))):\n",
        "                text = texts[i]\n",
        "                prediction = model(text.unsqueeze(0)).squeeze(1)\n",
        "                pred_label = torch.sigmoid(prediction).item()  # Применяем сигмоиду для получения вероятности\n",
        "                pred_label = 1 if pred_label > 0.5 else 0  # Классификация по порогу\n",
        "                tokens = [index_to_token.get(index.item(), '<unk>') for index in text if index != 0]\n",
        "                text_str = ' '.join(tokens)\n",
        "                examples.append((text_str, pred_label))\n",
        "                if len(examples) >= num_examples:\n",
        "                    break\n",
        "            if len(examples) >= num_examples:\n",
        "                break\n",
        "\n",
        "    for i, (text, pred_label) in enumerate(examples):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(f\"Text: {text}\")\n",
        "        print(f\"Predicted Label: {pred_label}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Гиперпараметры\n",
        "embedding_dim = 50\n",
        "num_filters = 100\n",
        "filter_size = 3\n",
        "output_dim = 1  # Бинарная классификация\n",
        "dropout = 0.5\n",
        "max_len = 100  # Максимальная длина последовательности\n",
        "\n",
        "# Загрузка данных\n",
        "train_data = pd.read_csv('/content/sample_data/train.csv', header=0, names=['id', 'label', 'tweet'], dtype={'tweet': str})\n",
        "test_data = pd.read_csv('/content/sample_data/test.csv', header=0, names=['id', 'tweet'], dtype={'tweet': str})\n",
        "\n",
        "# Создаем словарь на основе тренировочных данных\n",
        "vocab = build_vocab(train_data['tweet'], max_size=10000)\n",
        "index_to_token = get_index_to_token(vocab)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Инициализация модели, оптимизатора и критерия\n",
        "model = SentimentModel(vocab_size, embedding_dim, num_filters, filter_size, output_dim, dropout)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Создание датасетов и загрузчиков данных\n",
        "train_dataset = TrainTextDataset(csv_file='/content/sample_data/train.csv', vocab=vocab, max_len=max_len)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = TestTextDataset(csv_file='/content/sample_data/test.csv', vocab=vocab, max_len=max_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Обучение модели\n",
        "def train_model(model, data_loader, optimizer, criterion, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for texts, labels in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs.squeeze(1), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader):.4f}')\n",
        "\n",
        "train_model(model, train_loader, optimizer, criterion, num_epochs=5)\n",
        "\n",
        "# Вывод примеров и предсказаний\n",
        "show_predictions(model, test_loader, index_to_token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Что влияет на точность: \n",
        "\n",
        "Увеличение объема данных:\n",
        "\n",
        " Большее количество данных для тренировки обычно приводит к лучшей обобщающей способности модели.\n",
        "\n",
        "Предварительная обработка текста:\n",
        "\n",
        "Удаление стоп-слов, пунктуации, приведение к нижнему регистру и лемматизация/стемминг могли бы улучшить представление текста для модели.\n",
        "\n",
        "Использование более качественного словаря (например, Word2Vec или GloVe embeddings) для представления слов вместо использования произвольного словаря с ограниченной длиной.\n",
        "\n",
        "Оптимизация гиперпараметров:\n",
        "\n",
        "Увеличение числа фильтров в свертке (Conv1D) может позволить модели лучше распознавать сложные паттерны в данных.\n",
        "Использование более сложной архитектуры, такой как двуслойные или многослойные RNN или LSTM, может повысить способность модели запоминать контекст.\n",
        "\n",
        "Регуляризация:\n",
        "\n",
        "Dropout помогает бороться с переобучением, но его гиперпараметры (например, вероятность отключения нейронов) могут быть оптимизированы.\n",
        "Аугментация данных:\n",
        "\n",
        "Для текстовых данных можно использовать методы, такие как переводы, перефразирование или синтаксическая трансформация, чтобы увеличить объем данных."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
